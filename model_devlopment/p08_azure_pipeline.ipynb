{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2b85c7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Connection-to-Azure-ws\" data-toc-modified-id=\"Connection-to-Azure-ws-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Connection to Azure ws</a></span></li><li><span><a href=\"#Define-Datastore\" data-toc-modified-id=\"Define-Datastore-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Define Datastore</a></span></li><li><span><a href=\"#Create-environment\" data-toc-modified-id=\"Create-environment-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create environment</a></span></li><li><span><a href=\"#Compute-cluster\" data-toc-modified-id=\"Compute-cluster-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Compute cluster</a></span></li><li><span><a href=\"#Script\" data-toc-modified-id=\"Script-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Script</a></span></li><li><span><a href=\"#Pipiline\" data-toc-modified-id=\"Pipiline-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Pipiline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Config\" data-toc-modified-id=\"Config-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Config</a></span></li><li><span><a href=\"#Step\" data-toc-modified-id=\"Step-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step</a></span></li><li><span><a href=\"#build-pipeline\" data-toc-modified-id=\"build-pipeline-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>build pipeline</a></span></li><li><span><a href=\"#Examine\" data-toc-modified-id=\"Examine-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Examine</a></span></li><li><span><a href=\"#Publish-pipeline\" data-toc-modified-id=\"Publish-pipeline-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Publish pipeline</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83217fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (pyOpenSSL 20.0.1 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pyopenssl<20.0.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (numpy 1.20.2 (d:\\anaconda\\lib\\site-packages), Requirement.parse('numpy<=1.19.3; sys_platform == \"win32\"'), {'azureml-dataset-runtime'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (pyOpenSSL 20.0.1 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pyopenssl<20.0.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (pyOpenSSL 20.0.1 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pyopenssl<20.0.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (pyOpenSSL 20.0.1 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pyopenssl<20.0.0'), {'azureml-core'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (pyOpenSSL 20.0.1 (d:\\anaconda\\lib\\site-packages), Requirement.parse('pyopenssl<20.0.0')).\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4601db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Environment, ScriptRunConfig, Experiment\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd4726",
   "metadata": {},
   "source": [
    "## Connection to Azure ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af7b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.19.0 to work with projet_7\n"
     ]
    }
   ],
   "source": [
    "#import workspace from config.json\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced61da",
   "metadata": {},
   "source": [
    "## Define Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457e371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec45fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be023e22",
   "metadata": {},
   "source": [
    "## Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7067ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create environment from yml file\n",
    "env_p8 = Environment.from_conda_specification(\"env_p8\", 'env-p8.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0e4d6",
   "metadata": {},
   "source": [
    "## Compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cd9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "cluster_name = 'cluster-projet7'\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e731ce",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e45cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_azure/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_azure/data_prep.py\n",
    "\n",
    "print('print importing lib...')\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "from azureml.core import Dataset\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "import random\n",
    "import implicit\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "print('lib imported...')\n",
    "\n",
    "#get scripts arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data', type=str, dest='dataset_folder')\n",
    "#outfolder for preprocessed data to use in pipeline. see OutputFileDatasetConfig doc from azure\n",
    "parser.add_argument('--prepped_data', type=str, dest='prepped_data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "save_folder = args.prepped_data\n",
    "\n",
    "#set parameters\n",
    "\n",
    "\n",
    "#get the experiment run context and workspace\n",
    "run = Run.get_context()\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "#load data\n",
    "print('loading data...')\n",
    "#frame = Dataset.get_by_name(ws, dataset_name).to_pandas_dataframe()\n",
    "# Get the training data path from the input. must be passed as argument in the script as as_named_input + as_download or as_mount\n",
    "data_path = run.input_datasets['clicks']\n",
    "#import in dataframe\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "frame = pd.concat((pd.read_csv(f) for f in all_files), axis=0, ignore_index=True)\n",
    "print('data loaded...')\n",
    "\n",
    "################################################################################################################################\n",
    "#                FUNCTION DEFINITION\n",
    "################################################################################################################################\n",
    "\n",
    "def make_train_2(ratings, pct_test = 0.2):\n",
    "    '''\n",
    "    Function take original user_article matrix choose a percentage of random user and mask one article \n",
    "    per user selected. it returns the original matrix, the new matrix and a dictionary of the masked pair\n",
    "    user article\n",
    "    \n",
    "    '''\n",
    "     # Make a copy of the original set to be the test set.\n",
    "    test_set = ratings.copy() \n",
    "    # Store the test set as a binary preference matrix\n",
    "    test_set[test_set != 0] = 1 \n",
    "    # Make a copy of the original data we can alter as our training set.\n",
    "    training_set = ratings.copy()  \n",
    "    \n",
    "    # Find the indices in the ratings data where an interaction exists\n",
    "    nonzero_inds = training_set.nonzero() \n",
    "    # Zip these pairs together of user,item index into list\n",
    "    nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) \n",
    "    \n",
    "    \n",
    "    random.seed(0) \n",
    "    # Round the number of samples needed to the nearest integer \n",
    "    num_samples = int(np.ceil(pct_test*training_set.shape[0])) \n",
    "    # Sample a random number of user without replacement\n",
    "    sample_user = random.sample(set(list(nonzero_inds[0])), num_samples)\n",
    "    \n",
    "    # selec one random article per user\n",
    "    item_ind=[]\n",
    "    for user in sample_user:\n",
    "        list_artic_user = [index[1] for index in nonzero_pairs if index[0]==user]\n",
    "        article_hide = random.sample(list_artic_user, 1)\n",
    "        item_ind.extend(article_hide) \n",
    "    \n",
    "    # Assign all of the randomly chosen user-item pairs to zero\n",
    "    training_set[sample_user, item_ind] = 0 \n",
    "    # Get rid of zeros in sparse array storage after update to save space\n",
    "    training_set.eliminate_zeros()\n",
    "    \n",
    "    #dictionary of pairs\n",
    "    user_item_hide = dict(zip(sample_user, item_ind))\n",
    "    \n",
    "    # Output the unique list of user rows that were altered  \n",
    "    return training_set, test_set, user_item_hide\n",
    "\n",
    "#############################################################################################################\n",
    "#                DATA PREP\n",
    "#############################################################################################################\n",
    "print('start preperation...')\n",
    "#keep column of interest\n",
    "#after extracting from csv dtype = object with must be turn into int or float for csr matrix\n",
    "user_article = frame[['user_id', 'click_article_id']]\n",
    "\n",
    "\n",
    "#make sparse matric : np.ones_like to put the wieght at one (read or not read) could be rating if existed \n",
    "matrix_user_article = sparse.csc_matrix((np.ones_like(user_article['user_id'].astype(int)), (user_article['user_id'].astype(int), user_article['click_article_id'].astype(int))))\n",
    "print('sparse matrix ok')\n",
    "\n",
    "#make train test set\n",
    "train, test, user_item_altered = make_train_2(matrix_user_article, pct_test = 0.2)\n",
    "\n",
    "#save the prepped data\n",
    "print('saving data..')\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "train_path = os.path.join(save_folder, 'train.npz')\n",
    "test_path = os.path.join(save_folder, 'test.npz')\n",
    "user_item_path = os.path.join(save_folder, 'user_item_altered.pkl')\n",
    "\n",
    "sparse.save_npz(train_path, train)\n",
    "sparse.save_npz(test_path, test)\n",
    "joblib.dump(user_item_altered, user_item_path)\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b11c5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'recommender_1630613711_8b74f22b',\n",
       " 'target': 'cluster-projet7',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2021-09-02T20:21:40.380602Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '1bab7fe5-2941-4fbe-8dee-9da00609ab5b',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': '4a90605f-9d36-45c2-9062-2291f3293474'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'clicks', 'mechanism': 'Download'}}],\n",
       " 'outputDatasets': [{'identifier': {'savedId': '4c35184c-4b64-438c-b0dd-22e9120ce69a'},\n",
       "   'outputType': 'RunOutput',\n",
       "   'outputDetails': {'outputName': 'prepped_data'},\n",
       "   'dataset': {\n",
       "     \"source\": [\n",
       "       \"('workspaceblobstore', 'dataset/recommender_1630613711_8b74f22b/prepped_data/')\"\n",
       "     ],\n",
       "     \"definition\": [\n",
       "       \"GetDatastoreFiles\"\n",
       "     ],\n",
       "     \"registration\": {\n",
       "       \"id\": \"4c35184c-4b64-438c-b0dd-22e9120ce69a\",\n",
       "       \"name\": null,\n",
       "       \"version\": null,\n",
       "       \"workspace\": \"Workspace.create(name='projet_7', subscription_id='b9053cbf-be55-4e83-8c03-d6b0eb90cb5a', resource_group='Projet_7')\"\n",
       "     }\n",
       "   }}],\n",
       " 'runDefinition': {'script': 'data_prep.py',\n",
       "  'command': '',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--prepped_data',\n",
       "   'DatasetOutputConfig:prepped_data',\n",
       "   '--input-data',\n",
       "   'DatasetConsumptionConfig:clicks'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'cluster-projet7',\n",
       "  'dataReferences': {},\n",
       "  'data': {'clicks': {'dataLocation': {'dataset': {'id': '4a90605f-9d36-45c2-9062-2291f3293474',\n",
       "      'name': 'clicks_sample',\n",
       "      'version': '1'},\n",
       "     'dataPath': None,\n",
       "     'uri': None},\n",
       "    'mechanism': 'Download',\n",
       "    'environmentVariableName': 'clicks',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False,\n",
       "    'options': None}},\n",
       "  'outputData': {'prepped_data': {'outputLocation': None,\n",
       "    'mechanism': 'Mount',\n",
       "    'additionalOptions': {'pathOnCompute': None,\n",
       "     'registrationOptions': {'name': None,\n",
       "      'description': None,\n",
       "      'tags': None,\n",
       "      'datasetRegistrationOptions': {'additionalTransformation': None}},\n",
       "     'uploadOptions': None,\n",
       "     'mountOptions': None},\n",
       "    'environmentVariableName': None}},\n",
       "  'datacaches': [],\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': 2592000,\n",
       "  'nodeCount': 1,\n",
       "  'priority': None,\n",
       "  'credentialPassthrough': False,\n",
       "  'identity': None,\n",
       "  'environment': {'name': 'env_p8',\n",
       "   'version': 'Autosave_2021-09-01T15:58:59Z_a3dd4a10',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['conda-forge', 'defaults'],\n",
       "     'dependencies': ['pip',\n",
       "      'pandas',\n",
       "      'scikit-learn',\n",
       "      'scipy',\n",
       "      'python=3.8',\n",
       "      'numpy',\n",
       "      'ipykernel',\n",
       "      'notebook',\n",
       "      {'pip': ['azureml-core',\n",
       "        'azureml-dataset-runtime',\n",
       "        'azureml-defaults',\n",
       "        'implicit',\n",
       "        'joblib']}],\n",
       "     'name': 'azureml_62cf9252c4499e32ea7ce8abcddf458c'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20200821.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': False,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'enableMLflowTracking': True,\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': None},\n",
       "  'aiSuperComputer': {'instanceType': 'D2',\n",
       "   'imageVersion': 'pytorch-1.7.0',\n",
       "   'location': None,\n",
       "   'aiSuperComputerStorageData': None,\n",
       "   'interactive': False,\n",
       "   'scalePolicy': None,\n",
       "   'virtualClusterArmId': None,\n",
       "   'tensorboardLogDirectory': None,\n",
       "   'sshPublicKey': None,\n",
       "   'enableAzmlInt': True,\n",
       "   'priority': 'Medium',\n",
       "   'slaTier': 'Standard',\n",
       "   'userAlias': None},\n",
       "  'kubernetesCompute': {'instanceType': None},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'pyTorch': {'communicationBackend': None, 'processCount': None},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': False,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
       "   'successfulReturnCodes': []},\n",
       "  'environmentVariables': {},\n",
       "  'applicationEndpoints': {},\n",
       "  'parameters': [],\n",
       "  'dataBricks': {'workers': 0,\n",
       "   'minimumWorkerCount': 0,\n",
       "   'maxMumWorkerCount': 0,\n",
       "   'sparkVersion': '4.0.x-scala2.11',\n",
       "   'nodeTypeId': 'Standard_D3_v2',\n",
       "   'sparkConf': {},\n",
       "   'sparkEnvVars': {},\n",
       "   'instancePoolId': None,\n",
       "   'timeoutSeconds': 0,\n",
       "   'jarLibraries': [],\n",
       "   'eggLibraries': [],\n",
       "   'whlLibraries': [],\n",
       "   'pypiLibraries': [],\n",
       "   'rCranLibraries': [],\n",
       "   'mavenLibraries': []}},\n",
       " 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/55_azureml-execution-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt?sv=2019-07-07&sr=b&sig=qC8QxWLuqWubx%2F%2FwRUn2i%2Fc89dGuA0Q4%2B7wfNfIrSx4%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/65_job_prep-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt?sv=2019-07-07&sr=b&sig=IV6u7p%2FFePk8%2FmDqZI49FmhKz86O0B0O3uB7WTPF2%2Fw%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/70_driver_log.txt?sv=2019-07-07&sr=b&sig=iiLk8MQgXCxCnNJEkdFosz6WTipVzC8PjmqCFV89IpY%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/75_job_post-tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p.txt?sv=2019-07-07&sr=b&sig=vfs3gPA7wExG5qaEkmRO5q9D8NpgPg4ZZCaBw%2FpIauc%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/process_info.json?sv=2019-07-07&sr=b&sig=JJJhLVgU1IfipLRXTvm%2FScc3oL6y99N5H5C%2BvGdbYS0%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/azureml-logs/process_status.json?sv=2019-07-07&sr=b&sig=BHXTXZ7ykBbmmcINu1y33WDnACtrBry7ueqvt2lnfyA%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/86_azureml.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/86_azureml.log?sv=2019-07-07&sr=b&sig=IBEL2l6bZeX9FXOD7cld%2B61dEkJWkfMu12%2FspoQIEZE%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=SoOUI1S8JngLD6635ejlT2TAcYnmCzSKYGoCINA7w9Y%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=Qm%2BBPqPYfJDD%2F%2FBwwCL8ma6VC5oUFXKXnJDBvyonr2o%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/job_prep_azureml.log?sv=2019-07-07&sr=b&sig=wB5wVI1sskraup%2FFMd2iL1rIxZQ4JKN6RrqgYTr6Zl8%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/job_release_azureml.log?sv=2019-07-07&sr=b&sig=rV2goTUpCgIkiY%2F6XQT1SSbcqBGRFE6t6w66jiDz8D4%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/all.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/all.log?sv=2019-07-07&sr=b&sig=2%2BfQTv%2FPxxlOfZFTn4%2BCqdph134VB2iSwkug4t3TWxc%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/task.enter_contexts.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/task.enter_contexts.log?sv=2019-07-07&sr=b&sig=c26HpliqM4D0VmTgfR1XPk%2B8kRZMbasdXhP2YsXbWII%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r',\n",
       "  'logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/task.exit_contexts.log': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.recommender_1630613711_8b74f22b/logs/azureml/sidecar/tvmps_089c8877e8ce1e32aab2e0256351ce0e2003b79fc73b24a70ae86b5ff1d9266e_p/task.exit_contexts.log?sv=2019-07-07&sr=b&sig=hTofZXeHeCcgpUCnTd5TC5fT5BBQyPxcMz%2BrmPw%2BIF0%3D&st=2021-09-02T20%3A13%3A45Z&se=2021-09-03T04%3A23%3A45Z&sp=r'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the training dataset\n",
    "clicks_ds = ws.datasets.get(\"clicks_sample\")\n",
    "\n",
    "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
    "#with no specification in OutputFileDatasetConfig for destination it will be in workspaceblobstore datastore\n",
    "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
    "\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory='training_azure',\n",
    "                                script='data_prep.py',\n",
    "                                arguments = ['--prepped_data', prepped_data, # Regularizaton rate parameter\n",
    "                                             '--input-data', clicks_ds.as_named_input('clicks').as_download()], # Reference to dataset location\n",
    "                                environment=env_p8,\n",
    "                                compute_target=cluster_name) \n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'recommender'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "#RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00b5b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_azure/train_recommender.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_azure/train_recommender.py\n",
    "\n",
    "print('print importing lib...')\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Run, Model\n",
    "from azureml.core import Dataset\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "import random\n",
    "import implicit\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "print('lib imported...')\n",
    "\n",
    "#get scripts arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--training-data\", type=str, dest='training_data', help='training data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "save_folder = args.training_data\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print('loading data..')\n",
    "\n",
    "train_path = os.path.join(save_folder, 'train.npz')\n",
    "test_path = os.path.join(save_folder, 'test.npz')\n",
    "user_item_path = os.path.join(save_folder, 'user_item_altered.pkl')\n",
    "\n",
    "train = sparse.load_npz(train_path)\n",
    "test = sparse.load_npz(test_path)\n",
    "user_item_altered = joblib.load(user_item_path)\n",
    "\n",
    "print('data loaded..')\n",
    "\n",
    "##################################################################################################\n",
    "#                FUNCTION DEFINITION\n",
    "##################################################################################################\n",
    "def predict_evaluate(model, train, user_item_altered):\n",
    "    '''\n",
    "    This function make 10 predictions for every users that had an article hiden during the make train process.\n",
    "    It then evaluate if the hidden article is included in the 10 predictions.\n",
    "    It calculate the regular hit nbr_of_hit / nbr_of_user_altered.\n",
    "    And it calculates the second matrix which take into account the position of the hidden artcile in the \n",
    "    prediction list. \n",
    "    \n",
    "    Metrics : hit rate and average reciprocal hit rank\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    hit = 0\n",
    "    sum_rev_pos = 0\n",
    "    for key, value in user_item_altered.items():\n",
    "        #make recommendation for each altered user\n",
    "        recommendation = model.recommend(key, train)\n",
    "        #store in list\n",
    "        recommended_item = [index[0] for index in recommendation]\n",
    "\n",
    "        #check if hiden article is in the recommendation list. calculate hit rate (HR) and average reciprocal\n",
    "        #hit rank (ARHR)\n",
    "\n",
    "        if user_item_altered[key] in recommended_item:\n",
    "            #number of hit\n",
    "            hit+=1\n",
    "            \n",
    "            #get positon of the hit in the recommendation list\n",
    "            pos = recommended_item.index(user_item_altered[key])+1\n",
    "            sum_rev_pos = sum_rev_pos+(1/pos)\n",
    "\n",
    "    #hit rate\n",
    "    HR = hit/(len(user_item_altered))\n",
    "\n",
    "    #average reciprocal hit rank\n",
    "    f = 1/len(user_item_altered)\n",
    "    ARHR = f*sum_rev_pos\n",
    "\n",
    "    return HR, ARHR\n",
    "\n",
    "##################################################################################################\n",
    "#                TRAINING\n",
    "##################################################################################################\n",
    "\n",
    "#Build model from implicit librairy\n",
    "# factors : numbers of laten factors to compute\n",
    "# regularization : reg factor\n",
    "#iterations : number of ALS iteraction to use when fitting data\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "\n",
    "#to fit the model we need the transpose from the train matrix i.e article_user\n",
    "train_T = train.transpose()\n",
    "\n",
    "print(\"Fitting..\")\n",
    "#fit model on csr matrix\n",
    "model.fit(train_T)\n",
    "\n",
    "print('Evaluate..')\n",
    "#evaluate with the original csr matrix \n",
    "HR, ARHR = predict_evaluate(model, train, user_item_altered)\n",
    "\n",
    "print(\"Hit Rate of:\", HR)\n",
    "print(\"Average Reciprocal Hit Rate of:\", ARHR)\n",
    "run.log('HR', np.float(HR))\n",
    "run.log('ARHR', np.float(ARHR))\n",
    "\n",
    "#save the trained model in the outputs folder\n",
    "print(\"saving model..\")\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "model_file = os.path.join('outputs', 'recommender_model.pkl')\n",
    "joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "#register the model\n",
    "Model.register(workspace=run.experiment.workspace,\n",
    "              model_path = model_file,\n",
    "              model_name = 'recommender_model',\n",
    "              properties={'Hit Rate': np.float(HR),\n",
    "                         \"Average Reciprocal Hit Rate\": np.float(ARHR)})\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f832c",
   "metadata": {},
   "source": [
    "## Pipiline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec948c5",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bdcac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9f480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment folder\n",
    "experiment_folder = 'training_azure'\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "#assign compute\n",
    "pipeline_run_config.target = training_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = env_p8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407ad62",
   "metadata": {},
   "source": [
    "### Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bafc3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ef947f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class OutputFileDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n",
      "Class OutputDatasetConfig: This is an experimental class, and may change at any time.<br/>For more information, see https://aka.ms/azuremlexperimental.\n"
     ]
    }
   ],
   "source": [
    "# Get the training dataset\n",
    "clicks_ds = ws.datasets.get(\"clicks\")\n",
    "\n",
    "# Create an OutputFileDatasetConfig (temporary Data Reference) for data passed from step 1 to step 2\n",
    "#with no specification in OutputFileDatasetConfig for destination it will be in workspaceblobstore datastore\n",
    "prepped_data = OutputFileDatasetConfig(\"prepped_data\")\n",
    "\n",
    "#step1, run the data prep script\n",
    "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"data_prep.py\",\n",
    "                                arguments = ['--prepped_data', prepped_data, \n",
    "                                             '--input-data', clicks_ds.as_named_input('clicks').as_download()],\n",
    "                                compute_target = training_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)\n",
    "\n",
    "# Step 2, run the training script\n",
    "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
    "                                source_directory = experiment_folder,\n",
    "                                script_name = \"train_recommender.py\",\n",
    "                                arguments = ['--training-data', prepped_data.as_input()],\n",
    "                                compute_target = training_cluster,\n",
    "                                runconfig = pipeline_run_config,\n",
    "                                allow_reuse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc1b3c",
   "metadata": {},
   "source": [
    "### build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8953ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297b6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Prepare Data [8ab72f12][49691165-a2c8-4f0e-ba78-fb78bba77b65], (This step will run and generate new outputs)\n",
      "Created step Train and Register Model [d0dc49fa][b5959ff3-b948-4c08-8b53-0d12b28d598f], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 50babf8e-e9b1-4da8-89a9-8fd79a10275b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/recommender-pipeline/runs/50babf8e-e9b1-4da8-89a9-8fd79a10275b?wsid=/subscriptions/b9053cbf-be55-4e83-8c03-d6b0eb90cb5a/resourcegroups/Projet_7/workspaces/projet_7\n",
      "PipelineRunId: 50babf8e-e9b1-4da8-89a9-8fd79a10275b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/recommender-pipeline/runs/50babf8e-e9b1-4da8-89a9-8fd79a10275b?wsid=/subscriptions/b9053cbf-be55-4e83-8c03-d6b0eb90cb5a/resourcegroups/Projet_7/workspaces/projet_7\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected a StepRun object but received <class 'azureml.core.run.Run'> instead.\n",
      "This usually indicates a package conflict with one of the dependencies of azureml-core or azureml-pipeline-core.\n",
      "Please check for package conflicts in your python environment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '50babf8e-e9b1-4da8-89a9-8fd79a10275b', 'status': 'Completed', 'startTimeUtc': '2021-09-02T21:15:35.347787Z', 'endTimeUtc': '2021-09-03T00:45:23.976211Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.50babf8e-e9b1-4da8-89a9-8fd79a10275b/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=xjFwd%2F6xbeZhFXA1fBhlLS5ToPkuTR10xaXosk30ib0%3D&skoid=781a4368-8669-4ef7-a3f9-be394693522d&sktid=33e47288-d1e1-43e8-b65b-4ba7bfd37a9f&skt=2021-09-02T20%3A50%3A05Z&ske=2021-09-03T18%3A57%3A55Z&sks=b&skv=2019-07-07&st=2021-09-03T04%3A26%3A00Z&se=2021-09-03T12%3A36%3A00Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.50babf8e-e9b1-4da8-89a9-8fd79a10275b/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=R%2Bqd6tP3TH213IMj6tk1Eul%2B1ssr49Njf5ZVDz3Y4oE%3D&skoid=781a4368-8669-4ef7-a3f9-be394693522d&sktid=33e47288-d1e1-43e8-b65b-4ba7bfd37a9f&skt=2021-09-02T20%3A50%3A05Z&ske=2021-09-03T18%3A57%3A55Z&sks=b&skv=2019-07-07&st=2021-09-03T04%3A26%3A00Z&se=2021-09-03T12%3A36%3A00Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://stockageprojet7opcr.blob.core.windows.net/azureml/ExperimentRun/dcid.50babf8e-e9b1-4da8-89a9-8fd79a10275b/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=LW0T%2F9YFPQBF4r6LCY6DG32dw%2FUDZ8mYAfzhU%2BDyRNI%3D&skoid=781a4368-8669-4ef7-a3f9-be394693522d&sktid=33e47288-d1e1-43e8-b65b-4ba7bfd37a9f&skt=2021-09-02T20%3A50%3A05Z&ske=2021-09-03T18%3A57%3A55Z&sks=b&skv=2019-07-07&st=2021-09-03T04%3A26%3A00Z&se=2021-09-03T12%3A36%3A00Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the pipeline\n",
    "pipeline_steps = [prep_step, train_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace=ws, name = 'recommender-pipeline')\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
    "\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d503f",
   "metadata": {},
   "source": [
    "### Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f78a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Register Model :\n",
      "\t HR : 0.238649736760607\n",
      "\t ARHR : 0.09021601729367221\n",
      "Prepare Data :\n"
     ]
    }
   ],
   "source": [
    "#run\n",
    "for run in pipeline_run.get_children():\n",
    "    print(run.name, ':')\n",
    "    metrics = run.get_metrics()\n",
    "    for metric_name in metrics:\n",
    "        print('\\t',metric_name, \":\", metrics[metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bc1fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommender_model version: 2\n",
      "\t Hit Rate : 0.238649736760607\n",
      "\t Average Reciprocal Hit Rate : 0.09021601729367221\n",
      "\n",
      "\n",
      "recommender_model version: 1\n",
      "\t Hit Rate : 0.056338028169014086\n",
      "\t Average Reciprocal Hit Rate : 0.017331768388106416\n",
      "\n",
      "\n",
      "glove_sample version: 4\n",
      "\t max_l : 45\n",
      "\n",
      "\n",
      "glove_sample version: 3\n",
      "\t max_l : 45\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model registered\n",
    "from azureml.core import Model\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6f54c",
   "metadata": {},
   "source": [
    "### Publish pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd69e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
